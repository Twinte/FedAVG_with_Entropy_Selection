{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Client 1 Label Entropy: 1.4341\n",
      "Client 2 Label Entropy: 0.7254\n",
      "Client 3 Label Entropy: 0.4863\n",
      "Client 4 Label Entropy: 2.0024\n",
      "Client 5 Label Entropy: 0.3448\n",
      "Client 6 Label Entropy: 2.6808\n",
      "Client 7 Label Entropy: 1.4427\n",
      "Client 8 Label Entropy: 1.6299\n",
      "Client 9 Label Entropy: 1.7878\n",
      "Client 10 Label Entropy: 1.6300\n",
      "Client 1: 461 samples\n",
      "Client 2: 27 samples\n",
      "Client 3: 3761 samples\n",
      "Client 4: 226 samples\n",
      "Client 5: 2233 samples\n",
      "Client 6: 19453 samples\n",
      "Client 7: 2129 samples\n",
      "Client 8: 11862 samples\n",
      "Client 9: 3279 samples\n",
      "Client 10: 6567 samples\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import time\n",
    "import logging\n",
    "from collections import Counter\n",
    "import math\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "# Characteristics for logging\n",
    "num_clients = 10\n",
    "alpha = 0.1  # Dirichlet distribution parameter for non-IID\n",
    "iid_type = \"non-iid\" if alpha < 1 else \"iid\"  # Characterize dataset\n",
    "algorithm = \"fedavg\"\n",
    "date_str = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Log file name format: includes date, algorithm, non-IID/IID type, and number of clients\n",
    "log_file_name = f\"federated_learning_{algorithm}_{iid_type}_{num_clients}clients_{date_str}.txt\"\n",
    "\n",
    "# Ensure the directory for logs exists\n",
    "log_directory = './experiment_logs'  # You can change this path as needed\n",
    "os.makedirs(log_directory, exist_ok=True)  # Create directory if it doesn't exist\n",
    "log_file_path = os.path.join(log_directory, log_file_name)\n",
    "\n",
    "# Set up logging with dynamic file name using full log_file_path\n",
    "logging.basicConfig(filename=log_file_path, level=logging.INFO)\n",
    "\n",
    "# Client failure parameters\n",
    "failure_probability = 0.2  # 20% chance of failure per round\n",
    "rejoin_probability = 0.3   # 30% chance that a failed client will rejoin in the next round\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the transformations for the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Download and load CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# DataLoader for the test dataset\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Function to split dataset using Dirichlet distribution for non-IID split\n",
    "def split_dataset_by_dirichlet(dataset, num_clients, alpha):\n",
    "    indices = np.arange(len(dataset))\n",
    "    label_array = np.array([dataset[i][1] for i in indices])  # Get labels from the dataset\n",
    "    client_indices = [[] for _ in range(num_clients)]\n",
    "\n",
    "    # Split data for each class based on Dirichlet distribution\n",
    "    for label in np.unique(label_array):\n",
    "        class_indices = indices[label_array == label]\n",
    "        np.random.shuffle(class_indices)\n",
    "        class_split = np.random.dirichlet([alpha] * num_clients) * len(class_indices)\n",
    "        class_split = np.round(class_split).astype(int)\n",
    "        class_split = np.cumsum(class_split).astype(int)\n",
    "\n",
    "        start = 0\n",
    "        for client_id in range(num_clients):\n",
    "            client_indices[client_id].extend(class_indices[start:class_split[client_id]])\n",
    "            start = class_split[client_id]\n",
    "\n",
    "    # Return indices for each client\n",
    "    return client_indices\n",
    "\n",
    "# Function to calculate entropy given label counts\n",
    "def calculate_entropy(label_counts):\n",
    "    total_samples = sum(label_counts.values())\n",
    "    entropy = 0.0\n",
    "    for count in label_counts.values():\n",
    "        if count > 0:\n",
    "            p_i = count / total_samples\n",
    "            entropy -= p_i * math.log2(p_i)\n",
    "    return entropy\n",
    "\n",
    "# Non-IID data partition\n",
    "client_data_indices = split_dataset_by_dirichlet(train_dataset, num_clients, alpha)\n",
    "\n",
    "# Create a DataLoader for each client\n",
    "client_loaders = [DataLoader(Subset(train_dataset, indices), batch_size=64, shuffle=True) \n",
    "                  for indices in client_data_indices]\n",
    "\n",
    "# Calculate and log entropy for each client\n",
    "for i, indices in enumerate(client_data_indices):\n",
    "    # Extract labels for the current client\n",
    "    labels = [train_dataset[idx][1] for idx in indices]\n",
    "    \n",
    "    # Count occurrences of each label\n",
    "    label_counts = Counter(labels)\n",
    "    \n",
    "    # Calculate entropy\n",
    "    entropy = calculate_entropy(label_counts)\n",
    "    \n",
    "    # Log the entropy\n",
    "    logging.info(f\"Client {i+1} Label Entropy: {entropy:.4f}\")\n",
    "    print(f\"Client {i+1} Label Entropy: {entropy:.4f}\")\n",
    "\n",
    "# Print number of samples per client (for validation)\n",
    "for i, loader in enumerate(client_loaders):\n",
    "    print(f\"Client {i+1}: {len(loader.dataset)} samples\")\n",
    "\n",
    "# Define the CNN architecture for CIFAR-10 (reduced complexity for optimization)\n",
    "class CIFAR10_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CIFAR10_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.001\n",
    "num_rounds = 30  # Reduced number of rounds for optimization\n",
    "num_local_epochs = 2  # Reduced number of local epochs\n",
    "\n",
    "# FedAvg Algorithm to average the model weights\n",
    "def fed_avg(global_model, client_models):\n",
    "    global_weights = global_model.state_dict()\n",
    "    for key in global_weights.keys():\n",
    "        global_weights[key] = torch.stack([client_models[i].state_dict()[key].float() for i in range(len(client_models))], dim=0).mean(dim=0)\n",
    "    global_model.load_state_dict(global_weights)\n",
    "    return global_model\n",
    "\n",
    "# Train each client locally\n",
    "def train_client(client_loader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in client_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "    accuracy = 100. * correct / total\n",
    "    return running_loss / len(client_loader), accuracy\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(test_loader, model, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    accuracy = 100. * correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Track active clients (start with all clients active)\n",
    "client_active_status = [True] * num_clients  # Initially, all clients are active\n",
    "\n",
    "# Main federated learning loop\n",
    "global_model = CIFAR10_CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Initialize log\n",
    "logging.info(\"Starting federated learning with 10 clients\")\n",
    "\n",
    "for round in range(num_rounds):\n",
    "    start_time = time.time()\n",
    "    logging.info(f\"Communication Round {round + 1}\")\n",
    "    \n",
    "    client_models = []\n",
    "    round_loss = []\n",
    "    round_accuracy = []\n",
    "\n",
    "    # Local training on each client\n",
    "    for client_id in range(num_clients):\n",
    "        # Simulate client failure\n",
    "        if client_active_status[client_id]:  # Client is active\n",
    "            if random.random() < failure_probability:\n",
    "                client_active_status[client_id] = False  # Client fails (quits)\n",
    "                logging.info(f\"Client {client_id + 1} failed and will not participate in this round.\")\n",
    "                continue  # Skip the training for this client\n",
    "        else:  # Client is currently inactive (failed previously)\n",
    "            if random.random() < rejoin_probability:\n",
    "                client_active_status[client_id] = True  # Client rejoins\n",
    "                logging.info(f\"Client {client_id + 1} rejoins the training in this round.\")\n",
    "\n",
    "        # If the client is active, proceed with training\n",
    "        if client_active_status[client_id]:\n",
    "            local_model = CIFAR10_CNN().to(device)\n",
    "            local_model.load_state_dict(global_model.state_dict())  # Start from global model\n",
    "            optimizer = optim.Adam(local_model.parameters(), lr=learning_rate)\n",
    "\n",
    "            # Training each client\n",
    "            client_loss, client_accuracy = train_client(client_loaders[client_id], local_model, criterion, optimizer, device)\n",
    "            round_loss.append(client_loss)\n",
    "            round_accuracy.append(client_accuracy)\n",
    "\n",
    "            client_models.append(local_model)\n",
    "            logging.info(f\"Client {client_id + 1} - Loss: {client_loss:.4f}, Accuracy: {client_accuracy:.2f}%\")\n",
    "\n",
    "    # Aggregate client models into the global model\n",
    "    if client_models:\n",
    "        global_model = fed_avg(global_model, client_models)\n",
    "    else:\n",
    "        logging.info(\"No clients participated in this round.\")\n",
    "\n",
    "    # Evaluate global model on the test set\n",
    "    test_accuracy = evaluate_model(test_loader, global_model, device)\n",
    "    logging.info(f\"Test Accuracy after round {round + 1}: {test_accuracy:.2f}%\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    logging.info(f\"Round {round + 1} completed in {elapsed_time:.2f} seconds\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
